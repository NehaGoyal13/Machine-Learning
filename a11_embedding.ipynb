{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS549 Machine Learning\n",
    "# Assignment 11: Word embedding model\n",
    "\n",
    "**Created by:** Yang Xu, Assistant Professor of Computer Science, San Diego State University\n",
    "\n",
    "**Total points: 10**\n",
    "\n",
    "In this assignment, you will exercise using pre-trained word embeddings for word semantic tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Cosine similarity and Euclidean distance\n",
    "\n",
    "**Points: 2**\n",
    "\n",
    "First, you can load the pre-trained embeddings using `torchtext` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Neha/.vector_cache/glove.6B.zip: 862MB [02:39, 5.39MB/s]                 \n",
      "100%|███████████████████████████████▉| 399999/400000 [00:09<00:00, 42823.91it/s]\n",
      "/Users/Neha/.vector_cache/wiki.en.vec: 6.60GB [03:56, 27.9MB/s]                 \n",
      "  0%|                                               | 0/2519370 [00:00<?, ?it/s]Skipping token b'2519370' with 1-dimensional vector [b'300']; likely a header\n",
      "100%|██████████████████████████████| 2519370/2519370 [04:09<00:00, 10096.68it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create a folder to save cached vectors.\n",
    "home_dir = os.path.expanduser('~')\n",
    "cache_dir = os.path.join(home_dir, '.vector_cache')\n",
    "if not os.path.exists(cache_dir):\n",
    "    os.mkdir(cache_dir)\n",
    "\n",
    "glove = torchtext.vocab.GloVe(name=\"6B\", # trained on Wikipedia 2014 corpus\n",
    "                              dim=50, # embedding size = 50\n",
    "                              cache=cache_dir # You can change it to a different directory where you wish to save the vectors\n",
    "                              )\n",
    "\n",
    "fasttext = torchtext.vocab.FastText(language='en',\n",
    "                                    cache=cache_dir) # The fasttext cache file is ~6GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the loaded pre-trained embeddings. You can use `.vectors` attribute to access the embedding matrix, which is a `torch.Tensor` of shape (vocab_size, embedding_size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([400000, 50])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([2519370, 300])\n"
     ]
    }
   ],
   "source": [
    "print(type(glove.vectors))\n",
    "print(glove.vectors.shape)\n",
    "\n",
    "print(type(fasttext.vectors))\n",
    "print(fasttext.vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Check out the word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50])\n",
      "torch.Size([300])\n"
     ]
    }
   ],
   "source": [
    "# print(glove['cat'])\n",
    "print(glove['cat'].shape)\n",
    "\n",
    "# print(fasttext['cat'])\n",
    "print(fasttext['cat'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "First, implement the function for computing Euclidean distance using `torch.norm()`. The function should return a `torch.Tensor`, so the `.item()` method in the testing code can retrieve the value for a scalar tensor.\n",
    "\n",
    "The Euclidean distance between two vectors $x=[x_1,x_2,...x_n]$  and $y=[y_1,y_2,...y_n]$ is the 2-norm of their difference x−y:\n",
    "\n",
    "$$Euclidean\\_distance = \\sqrt{\\sum_i(x_i - y_i)^2}$$\n",
    "\n",
    "Larger Euclidean distance between words means that they are more semantically apart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe distance:\n",
      "cat <-> dog: 1.8846\n",
      "cat <-> tree: 4.5569\n",
      "Fasttext distance:\n",
      "cat <-> dog: 3.4784\n",
      "cat <-> tree: 4.9178\n"
     ]
    }
   ],
   "source": [
    "### START YOUR CODE ###\n",
    "def euclid_dist(vec1, vec2) -> torch.Tensor:\n",
    "    return torch.norm(vec1 - vec2)\n",
    "### END YOUR CODE ###\n",
    "\n",
    "# Do not change the code below\n",
    "print('GloVe distance:')\n",
    "print('cat <-> dog: {:.4f}'.format(euclid_dist(glove['cat'], glove['dog']).item()))\n",
    "print('cat <-> tree: {:.4f}'.format(euclid_dist(glove['cat'], glove['tree']).item()))\n",
    "\n",
    "print('Fasttext distance:')\n",
    "print('cat <-> dog: {:.4f}'.format(euclid_dist(fasttext['cat'], fasttext['dog']).item()))\n",
    "print('cat <-> tree: {:.4f}'.format(euclid_dist(fasttext['cat'], fasttext['tree']).item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "\n",
    "GloVe distance:\\\n",
    "cat <-> dog: 1.8846\\\n",
    "cat <-> tree: 4.5569\n",
    "\n",
    "Fasttext distance:\\\n",
    "cat <-> dog: 3.4784\\\n",
    "cat <-> tree: 4.9178\n",
    "\n",
    "---\n",
    "\n",
    "The other metric, cosine similarity, measures the similarity rather than distance. Thus, larger cosine similarity score between two words indicates closer meanings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe distance:\n",
      "dog <-> cat: 0.9218\n",
      "cat <-> tree: 0.5661\n"
     ]
    }
   ],
   "source": [
    "# GloVe\n",
    "v1 = glove['dog'].unsqueeze(0)\n",
    "v2 = glove['cat'].unsqueeze(0)\n",
    "v3 = glove['tree'].unsqueeze(0)\n",
    "\n",
    "### START YOUR CODE ###\n",
    "s1 = torch.cosine_similarity(v1,v2) # Compute the cosine similarity between v1 and v2 using torch.cosine_similarity()\n",
    "s2 = torch.cosine_similarity(v2,v3) # between v2 and v3\n",
    "### END YOUR CODE ###\n",
    "\n",
    "print('GloVe distance:')\n",
    "print('dog <-> cat: {:.4f}'.format(s1.item()))\n",
    "print('cat <-> tree: {:.4f}'.format(s2.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "\n",
    "GloVe distance:\\\n",
    "dog <-> cat: 0.9218\\\n",
    "cat <-> tree: 0.5661\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fasttext distance:\n",
      "dog <-> cat: 0.6381\n",
      "cat <-> tree: 0.3314\n"
     ]
    }
   ],
   "source": [
    "# Fasttext\n",
    "v1 = fasttext['dog'].unsqueeze(0)\n",
    "v2 = fasttext['cat'].unsqueeze(0)\n",
    "v3 = fasttext['tree'].unsqueeze(0)\n",
    "\n",
    "### START YOUR CODE ###\n",
    "s1 = torch.cosine_similarity(v1,v2) # Compute the cosine similarity between v1 and v2 using torch.cosine_similarity()\n",
    "s2 = torch.cosine_similarity(v2,v3) # between v2 and v3\n",
    "### END YOUR CODE ###\n",
    "\n",
    "print('Fasttext distance:')\n",
    "print('dog <-> cat: {:.4f}'.format(s1.item()))\n",
    "print('cat <-> tree: {:.4f}'.format(s2.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "\n",
    "Fasttext distance:\\\n",
    "dog <-> cat: 0.6381\\\n",
    "cat <-> tree: 0.3314\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Nearest words in embedding space\n",
    "\n",
    "**Points: 4**\n",
    "\n",
    "Look through our entire vocabulary for words that are closest to a point in the embedding space\n",
    "\n",
    "You can use `embeddings.vectors` to access the entire embedding matrix for all the words. `torch.cosine_similarity()` can be applied to the entire embedding matrix and the target embedding, and the resulting similarity scores are between the target word and each word in the vocabulary (including the target word itself!). So, when you use `topk()` to pick the top `n` highest similar words, you need to use `n+1`, because the top 1st one is always the word itself.\n",
    "\n",
    "In the case for computing Euclidean distances, the top $n$ smallest similar words should be picked, and you can set `largest=False` in `torch.topk()`. Also **note** that use the correct `dim` parameter when calling `torch.norm()`.\n",
    "\n",
    "`torch.topk()` returns the top *k* **values** and their **indices** in a tensor. Refer to the document here: <https://pytorch.org/docs/stable/generated/torch.topk.html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_words(embeddings, target_word, method='cosine', n=5):\n",
    "    assert method in ['cosine', 'euclidean']\n",
    "\n",
    "    ### START YOUR CODE ###\n",
    "    target_emb = embeddings.vectors[embeddings.stoi[target_word]] # Get the embedding of target word\n",
    "    ### END YOUR CODE ###\n",
    "\n",
    "    if method == 'cosine':\n",
    "        ### START YOUR CODE ###\n",
    "        scores = torch.cosine_similarity(embeddings.vectors, target_emb.unsqueeze(0)) # Compute similarity scores between target word and all the words in vocabulary\n",
    "        values, indices = torch.topk(scores, n+1) # Hint: use torch.topk(), with n+1\n",
    "        ### END YOUR CODE ###\n",
    "    else:\n",
    "        ### START YOUR CODE ###\n",
    "        scores = torch.norm(embeddings.vectors - target_emb, dim=1) # Compute Euclidean distances between target word and all the words in vocabulary\n",
    "        values, indices = torch.topk(scores, n+1, largest=False) # Hint: use torch.topk(), with n+1\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    for val, idx in zip(values[1:], indices[1:]):\n",
    "        print('{}: {:.4f}'.format(embeddings.itos[idx], val.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the closest word to \"cat\", according to cosine similarity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove\n",
      "dog: 0.9218\n",
      "rabbit: 0.8488\n",
      "monkey: 0.8041\n",
      "rat: 0.7892\n",
      "cats: 0.7865\n",
      "\n",
      "Fasttext:\n",
      "cats: 3.2251\n",
      "dog: 3.4784\n",
      "kitten: 3.5513\n",
      "kittens: 3.6621\n",
      "fluffykittens: 3.7981\n"
     ]
    }
   ],
   "source": [
    "# Do not change the test code here\n",
    "# Glove\n",
    "print('Glove')\n",
    "nearest_words(glove, 'cat', n=5)\n",
    "\n",
    "# Fasttext\n",
    "print()\n",
    "print('Fasttext:')\n",
    "nearest_words(fasttext, 'cat', method='euclidean', n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "\n",
    "Glove\\\n",
    "dog: 0.9218\\\n",
    "rabbit: 0.8488\\\n",
    "monkey: 0.8041\\\n",
    "rat: 0.7892\\\n",
    "cats: 0.7865\n",
    "\n",
    "Fasttext:\\\n",
    "cats: 3.2251\\\n",
    "dog: 3.4784\\\n",
    "kitten: 3.5513\\\n",
    "kittens: 3.6621\\\n",
    "fluffykittens: 3.7981"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 3. Word analogies\n",
    "\n",
    "**Points: 4**\n",
    "\n",
    "Implement the classical word analogy task.\n",
    "\n",
    "You are given three words, for instance, w1 = *boy*, w2 = *girl*, w3 = *brother*, and you are to find which word is to *brother* as *girl* is to *boy*. In this example, the mostly likely answer is *sister*.\n",
    "\n",
    "Assume the embeddings for w1, w2, and w3 are e1, e2, and e3, respectively. You should use the subtraction $d = e2 - e1$, and add it to e3, $e3 + d$, as the target embedding to search for candidate words. You first compute the cosine similarity scores between this target embedding and all the word embeddings in vocabulary, and then find the top *n* top *n* candidate words using `torch.topk()`.\n",
    "\n",
    "**Note** that you should NOT use the `nearest_words()` function, because target embedding is not from a specifc word. But similar internal code can be adopted, including the detail of using `n+1` in `topk()`. This is because even if the target embedding is different from $e3$ for a small off $d$, its closest neighbour is still very likely to be w3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_analogy(embeddings, w1, w2, w3, n=5):\n",
    "    ### START YOUR CODE ###\n",
    "    e1 = embeddings.vectors[embeddings.stoi[w1]]\n",
    "    e2 = embeddings.vectors[embeddings.stoi[w2]]\n",
    "    e3 = embeddings.vectors[embeddings.stoi[w3]]\n",
    "    d = e2-e1\n",
    "    target_emb = e3+d\n",
    "\n",
    "    # cosine similarity\n",
    "    scores = torch.cosine_similarity(embeddings.vectors, target_emb.unsqueeze(0))\n",
    "    values, indices = torch.topk(scores, n+1)\n",
    "    ### END YOUR CODE ###\n",
    "\n",
    "    for val, idx in zip(values[1:], indices[1:]):\n",
    "        print('{}: {:.4f}'.format(embeddings.itos[idx], val.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queen: 0.8610\n",
      "queen: 0.6803\n",
      "mom: 0.6897\n",
      "tomorrow,: 0.6179\n"
     ]
    }
   ],
   "source": [
    "# Do not change the test code here\n",
    "word_analogy(glove, 'man', 'woman', 'king', n=1)\n",
    "\n",
    "word_analogy(fasttext, 'man', 'woman', 'king', n=1)\n",
    "\n",
    "word_analogy(fasttext, 'boy', 'girl', 'dad', n=1)\n",
    "\n",
    "word_analogy(fasttext, 'future', 'tomorrow', 'past', n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "\n",
    "queen: 0.8610\\\n",
    "queen: 0.6803\\\n",
    "mom: 0.6897\\\n",
    "tomorrow,: 0.6179"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
